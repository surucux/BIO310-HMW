# -*- coding: utf-8 -*-
"""BIO310_2021.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Nq9OAOqLlppLAu2UsRmTVXJBvFLUvxZi

#Data
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# %matplotlib inline

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv("/content/drive/MyDrive/datas/data.csv")
labels = pd.read_csv("/content/drive/MyDrive/datas/labels.csv")

data.head(3)

labels.head(3)

data.describe()

null_data = data.replace(0, np.nan, inplace=False)
null_data = null_data.isnull().sum()
null_data = null_data[null_data > 0]
null_data

"""We have quite many missing values.

###1. How many patients do we have data for?
"""

data = data.rename({'Unnamed: 0': 'Patients'}, axis=1) 
data.head(2)

data['Patients'].nunique()

"""Answer: We have data for 801 patients.

###2. How many genes are we measuring expression for?
"""

len(data.columns) - 1

"""Answer: We are measuring expression for 2531 genes.

###3. Plot a histogram of the mean gene expression with 10 bins.
"""

mean_expression = data.iloc[:,1:].mean().explode()
mean_expression

plt.hist(mean_expression, bins = 10, color = 'b', label='Mean Gene Expression')
plt.legend()

"""Observations: 

*   We have just a few mean gene expression greater than 12.5.
*   Most of the mean gene expressions varies from 7.5 to 11.25.
*   Mean gene expressions from 0 to 1.25 is considerably a lot. (Most probably due to missing values)

###4. Which gene has the maximum mean expression?
"""

mean_expression.sort_values(ascending = False).head(10)

"""Answer: gene_230 has the maximum mean expression.

###5. How many unique cancers do we have in the dataset?
"""

labels['Class'].nunique()

"""Answer: We have 5 unique cancers.

#K-means Clustering

##Part 1
"""

# It is better to normalize our data before clustering
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaled_values = scaler.fit_transform(data.iloc[:,1:])
data.iloc[:,1:] = scaled_values

data.head()

data.max()

data.min()

from sklearn.cluster import KMeans

sse_list = [] #Create a list to track the values of sum squared error for each value of k

kmeans = KMeans(n_clusters = 2, random_state = 1, init = 'k-means++')
kmeans.fit(data.iloc[:,1:])
min_sse = kmeans.inertia_

for epoch in range(4): # Run 4 more times. 
  kmeans.fit(data.iloc[:,1:])
  sse = kmeans.inertia_
  if sse < min_sse: # Take the best (minimum) sum squared error
    min_sse = sse

sse_list.append(min_sse)
min_sse

kmeans = KMeans(n_clusters = 3, random_state = 1, init = 'k-means++')
kmeans.fit(data.iloc[:,1:])
min_sse = kmeans.inertia_

for epoch in range(4): 
  kmeans.fit(data.iloc[:,1:])
  sse = kmeans.inertia_
  if sse < min_sse:
    min_sse = sse

sse_list.append(min_sse)
min_sse

kmeans = KMeans(n_clusters = 5, random_state = 1, init = 'k-means++')
kmeans.fit(data.iloc[:,1:])
min_sse = kmeans.inertia_

for epoch in range(4):
  kmeans.fit(data.iloc[:,1:])
  sse = kmeans.inertia_
  if sse < min_sse:
    min_sse = sse

sse_list.append(min_sse)
min_sse

kmeans = KMeans(n_clusters = 6, random_state = 1, init = 'k-means++')
kmeans.fit(data.iloc[:,1:])
min_sse = kmeans.inertia_

for epoch in range(4):
  kmeans.fit(data.iloc[:,1:])
  sse = kmeans.inertia_
  if sse < min_sse:
    min_sse = sse

sse_list.append(min_sse)
min_sse

kmeans = KMeans(n_clusters = 7, random_state = 1, init = 'k-means++')
kmeans.fit(data.iloc[:,1:])
min_sse = kmeans.inertia_

for epoch in range(4):
  kmeans.fit(data.iloc[:,1:])
  sse = kmeans.inertia_
  if sse < min_sse:
    min_sse = sse

sse_list.append(min_sse)
min_sse

kmeans = KMeans(n_clusters = 8, random_state = 1, init = 'k-means++')
kmeans.fit(data.iloc[:,1:])
min_sse = kmeans.inertia_

for epoch in range(4):
  kmeans.fit(data.iloc[:,1:])
  sse = kmeans.inertia_
  if sse < min_sse:
    min_sse = sse

sse_list.append(min_sse)
min_sse

kmeans = KMeans(n_clusters = 9, random_state = 1, init = 'k-means++')
kmeans.fit(data.iloc[:,1:])
min_sse = kmeans.inertia_

for epoch in range(4):
  kmeans.fit(data.iloc[:,1:])
  sse = kmeans.inertia_
  if sse < min_sse:
    min_sse = sse

sse_list.append(min_sse)
min_sse

num_of_clusters = [2,3,5,6,7,8,9]
plt.plot(num_of_clusters, sse_list, 'bx-')
plt.xlabel('Values of K')
plt.ylabel('Sum Squared Error')
plt.title('Elbow Plot')
plt.show()

"""###Conclusion
Optimal number of clusters should be 5 because the curve becomes 'relatively flatter' after this point.

##Part 2
"""

from sklearn.metrics import silhouette_score
range_n_clusters = [2, 3, 5, 6, 7, 8, 9]
best_score = -1 # As silhouette score has a range of [-1,1]
best_cluster = 2

for n_clusters in range_n_clusters:
  kmeans = KMeans(n_clusters=n_clusters, random_state = 1, init = 'k-means++')
  cluster_labels = kmeans.fit_predict(data.iloc[:,1:])
  silhouette_avg = silhouette_score(data.iloc[:,1:], cluster_labels)

  print("For n_clusters =", n_clusters,
          "The average silhouette_score is :", silhouette_avg)
  
  if silhouette_avg > best_score:
    best_score = silhouette_avg
    best_cluster = n_clusters

print("Best value of k =", best_cluster,
          "with the average silhouette_score of :", best_score)

"""###Conclusion
Does the best k (the optimal number of clusters) we have chosen match the true number of cancer types of the data?


> No, this does not match the true number of cancer types of the data as we have chosen 6 whereas the true number is 5. 




"""

from sklearn.metrics import silhouette_samples
import matplotlib.cm as cm

n_clusters = best_cluster
# Create a subplot with 1 row and 1 column
fig, ax1 = plt.subplots(1, 1)
fig.set_size_inches(18, 7)

# The 1st subplot is the silhouette plot

ax1.set_xlim([-0.2, 0.5])
# The (n_clusters+1)*10 is for inserting blank space between silhouette
# plots of individual clusters, to demarcate them clearly.
ax1.set_ylim([0, len(data.iloc[:,1:]) + (n_clusters + 1) * 10])

# Initialize the clusterer with n_clusters value and a random generator
# seed of 10 for reproducibility.
clusterer = KMeans(n_clusters=n_clusters, random_state = 1, init = 'k-means++')
cluster_labels = clusterer.fit_predict(data.iloc[:,1:])

# The silhouette_score gives the average value for all the samples.
# This gives a perspective into the density and separation of the formed
# clusters
silhouette_avg = silhouette_score(data.iloc[:,1:], cluster_labels)

# Compute the silhouette scores for each sample
sample_silhouette_values = silhouette_samples(data.iloc[:,1:], cluster_labels)

y_lower = 10
for i in range(n_clusters):
  # Aggregate the silhouette scores for samples belonging to
  # cluster i, and sort them
  ith_cluster_silhouette_values = \ sample_silhouette_values[cluster_labels == i]

  ith_cluster_silhouette_values.sort()

  size_cluster_i = ith_cluster_silhouette_values.shape[0]
  y_upper = y_lower + size_cluster_i

  color = cm.nipy_spectral(float(i) / n_clusters)
  ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)

  # Label the silhouette plots with their cluster numbers at the middle
  ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i+1))

  # Compute the new y_lower for next plot
  y_lower = y_upper + 10  # 10 for the 0 samples

  ax1.set_title("The silhouette plot for the various clusters.")
  ax1.set_xlabel("The silhouette coefficient values")
  ax1.set_ylabel("Cluster label")

  # The vertical line for average silhouette score of all the values
  ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

  ax1.set_yticks([])  # Clear the yaxis labels / ticks
  ax1.set_xticks([-0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.4, 0.5])

plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
                  "with k_clusters = %d" % n_clusters),
                 fontsize=14, fontweight='bold') 
plt.show()