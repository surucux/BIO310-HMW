# -*- coding: utf-8 -*-
"""BIO310_HW4_2021.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WsTU0RMpU_ShzH6XlgVonjHuglaJLU2D
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# %matplotlib inline

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv("/content/drive/MyDrive/datas/data.csv")
labels = pd.read_csv("/content/drive/MyDrive/datas/labels.csv")

"""#1. Clustering using k-means"""

data.head(2)

# It is better to scaling our data before clustering
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans
scaler = MinMaxScaler(feature_range=(-1,1))
scaled_values = scaler.fit_transform(data.iloc[:,1:])
data.iloc[:,1:] = scaled_values

data.rename(columns={"Unnamed: 0": 'Sample'}, inplace = True)
data.head(2)

kmeans = KMeans(n_clusters = 5, random_state = 1, init = 'k-means++')
kmeans.fit(data.iloc[:,1:])

"""#2. Reduce dimensions of data.csv

##sklearn PCA
"""

from sklearn.decomposition import PCA
pca = PCA(n_components=2)

data_reduced_pca = pca.fit_transform(scaled_values)

"""What is the ratio of variance explained by the two
components?

"""

exp_var_pca = pca.explained_variance_ratio_
exp_var_pca

cum_sum_eigenvalues_pca = np.cumsum(exp_var_pca)
cum_sum_eigenvalues_pca

plt.figure(figsize=(8, 8))
plt.bar(range(1,len(exp_var_pca)+1), exp_var_pca, alpha=0.5, align='center', label='Individual explained variance')
plt.step(range(1,len(cum_sum_eigenvalues_pca)+1), cum_sum_eigenvalues_pca, where='mid',label='Cumulative explained variance', color = 'crimson',  linewidth=5)
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal component')
plt.xlim(0.5,2.5)
plt.ylim(0,0.25)
plt.xticks(ticks = range(1,len(exp_var_pca)+1), labels = [1,2])
plt.legend(loc='best')
plt.tight_layout()
plt.show()

"""##sklearn tSNE"""

from sklearn.manifold import TSNE
tsne = TSNE(n_components = 2)

data_reduced_tsne = tsne.fit_transform(scaled_values)

"""#3. Visualize k-means predictions

##Get cluster number predictions
"""

predicted_labels = kmeans.predict(data.iloc[:,1:])
predicted_labels.min(), predicted_labels.max()

"""##Plot the results of PCA

"""

pcadf = pd.DataFrame(data_reduced_pca, columns=["component_1", "component_2"])

pcadf['predicted_label'] = predicted_labels
pcadf['true_label'] = labels['Class']
pcadf.head(2)

import seaborn as sns

plt.style.use("fivethirtyeight")
plt.figure(figsize=(10, 10))

scat = sns.scatterplot(
      "component_1",
      "component_2",
      s=80,
      data=pcadf,
      hue="predicted_label",
      style="true_label",
      palette="Set2",
  )

scat.set_title("Clustering Visualization using PCA")
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)
plt.show()

"""##Plot the results of tSNE"""

tsnedf = pd.DataFrame(data_reduced_tsne, columns=["component_1", "component_2"])

tsnedf['predicted_label'] = predicted_labels
tsnedf['true_label'] = labels['Class']
tsnedf.head(2)

plt.style.use("fivethirtyeight")
plt.figure(figsize=(10, 10))

scat = sns.scatterplot(
      "component_1",
      "component_2",
      s=80,
      data=tsnedf,
      hue="predicted_label",
      style="true_label",
      palette="Set2",
  )

scat.set_title("Clustering Visualization using tSNE")
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)
plt.show()

"""##What do you observe?
> tSNE demonstrates clusters seperately and distinctively, which make the graph more readible. 
> On the other hand, clusters are a bit nested in PCA, so cluster 
seperations are less distinguishable than tSNE.

##3D Visualization
"""

import plotly.express as px

pca_3 =  PCA(n_components=3).fit_transform(scaled_values)
pca_3[0]

df = pd.DataFrame(pca_3, columns=["component_1", "component_2", "component_3"])

fig = px.scatter_3d(df, x='component_1', y='component_2', z='component_3',
                    color=predicted_labels, symbol=labels['Class'],opacity=0.8)
fig.show()